{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark training for data engineers\n",
    "## 02. Data Ingestion\n",
    "\n",
    "### Goal\n",
    "\n",
    "* Create an XML file with the notebook and retrieve the content using PySpark. The output is written to a pickle to be used in the next notebook.\n",
    "* Creating two CSV files with the notebook and retrieve the content using PySpark. Output is written to a single pickle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highlights\n",
    "\n",
    "* `spark.wholeTextFiles()` reads one file or directory at once.\n",
    "* `rdd.saveAsPickleFile()` saves an RDD straight to a pickle that can be re-used.\n",
    "\n",
    "### Implementation\n",
    "For every Spark job, **always create a context**. This basically means you get one (or more) containers running a Spark job allocated for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "config = SparkConf().setMaster('local')\n",
    "spark = SparkContext.getOrCreate(conf=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the Spark context has been created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://142.44.184.35:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XML ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.xml\n"
     ]
    }
   ],
   "source": [
    "%%file test.xml\n",
    "<teststructure>\n",
    "    <info>testfile</info>\n",
    "    <outerlist>\n",
    "        <elt>\n",
    "            <subelement>One</subelement>\n",
    "        </elt>\n",
    "        <elt>\n",
    "            <subelement>Two</subelement>\n",
    "        </elt>\n",
    "    </outerlist>\n",
    "</teststructure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data object by reading the XML file into an RDD. By specifying `file://` we indicate we want to read from the local file system. Often Spark is used with Hadoop and the file is read from the Hadoop filesystem and will have an URL like `hdfs://`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check where we are working now to get the absolute path to the XML file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jitsejan/itility/pyspark-101\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.wholeTextFiles('file://///home/jitsejan/itility/pyspark-101/test.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to call `collect()` in order to see the content of the rdd, since it is a lazy execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file:/home/jitsejan/itility/pyspark-101/test.xml\n",
      "Content:\n",
      " <teststructure>\n",
      "    <info>testfile</info>\n",
      "    <outerlist>\n",
      "        <elt>\n",
      "            <subelement>One</subelement>\n",
      "        </elt>\n",
      "        <elt>\n",
      "            <subelement>Two</subelement>\n",
      "        </elt>\n",
      "    </outerlist>\n",
      "</teststructure>\n"
     ]
    }
   ],
   "source": [
    "for file in rdd.collect():\n",
    "    filename, filecontent = file\n",
    "    print('Found %s' % filename)\n",
    "    print('Content:\\n %s' % filecontent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the content is indeed the same as we specified on top of this notebook. Lets use the method `saveAsPickleFile` to save the RDD to a variable on disk which we can use in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.saveAsPickleFile('xml-pickle-file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting csvfile01.csv\n"
     ]
    }
   ],
   "source": [
    "%%file csvfile01.csv\n",
    "john,doe,male,32\n",
    "jake,doe,male,16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting csvfile02.csv\n"
     ]
    }
   ],
   "source": [
    "%%file csvfile02.csv\n",
    "jane,doe,female,31\n",
    "janet,doe,female,13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a wild card to read several files into the RDD at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvrdd = spark.wholeTextFiles('file://///home/jitsejan/itility/pyspark-101/*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the files that the reader picked up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:/home/jitsejan/itility/pyspark-101/csvfile01.csv\n",
      "file:/home/jitsejan/itility/pyspark-101/csvfile02.csv\n"
     ]
    }
   ],
   "source": [
    "for filename, filecontent in csvrdd.collect():\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the RDD to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvrdd.saveAsPickleFile('csv-pickle-file')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
